{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a1_Yulong_Wang_yw3367_HW1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PZihj-d_tEt",
        "colab_type": "text"
      },
      "source": [
        "**Assignment 1, Due 9/26**\n",
        "\n",
        "## About\n",
        "In this assignment, you will gain experience implementing a linear model, a neural network, and a deep neural network using TensorFlow 2.0. \n",
        "\n",
        "* You will use two different development styles. I thought it’d be helpful for you to see both of these early (if you’re familiar with them, you can branch out to any major framework that exist today). \n",
        "\n",
        "* Along the way, you'll add code to visualize the weights of a linear model, and provide your own implementation of softmax (so you learn to extend the built-in functionality right off the bat).\n",
        "\n",
        "This assignment has several parts, plan ahead and get started early (and come to office hours if you’re stuck, the TAs and I are happy to help). Most concepts will be covered in lecture two, just posting it early.\n",
        "\n",
        "## Instructions\n",
        "\n",
        "Complete the code in this notebook by searching for the text **\"TODO\"**.\n",
        "\n",
        "## Submission instructions\n",
        "\n",
        "Please submit this assignment on CourseWorks by uploading a Jupyter notebook that includes saved output. If you are working in Colab, you can prepare your notebook for submission by ensuring that runs end-to-end, then saving and downloading it:\n",
        "\n",
        "1. ```Runtime -> Restart and run all```\n",
        "1. ```File -> Save```\n",
        "1. ```File -> Download.ipynb```\n",
        "\n",
        "## Resources\n",
        "\n",
        "You can find all the latest tutorials for TensorFlow 2.0 [here](https://www.tensorflow.org/beta). Code examples that will help you with each part of the assignment are linked below.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ5ssDBBl9n6",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDj1_w6v-mYW",
        "colab_type": "text"
      },
      "source": [
        "### Install TensorFlow 2.0\n",
        "\n",
        "If you are running this notebook in Colab, the magic command below will install the most recent version. If you prefer working in Jupyter locally, you will need to install TensorFlow 2.0 on your system, using ```!pip install tensorflow==2.0.0-rc0```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1wARst_UkEi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8af2ea38-20e9-43ba-a2fb-4c30f466d5fb"
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYSALG-y-_7b",
        "colab_type": "text"
      },
      "source": [
        "### Check which TF version is installed\n",
        "TensorFlow 2.0 is currently under development. It's good practice to check which version you have installed. All the code you'll write in this course will be for version 2.0 (currently in beta)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21RQ0x71Ut_t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0d056b8f-ab9c-4b16-d68e-3185154771ba"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "assert tf.__version__.startswith('2')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQPA61YHUvht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Flatten \n",
        "from tensorflow.keras import Model\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vfWS0eYvzae",
        "colab_type": "text"
      },
      "source": [
        "## Part 1: First steps with Sequential models\n",
        "\n",
        "You will work with the Sequential API in this section. This is the easiest way to develop models with TF 2.0, and is the most common in practice. \n",
        "\n",
        "Here are a few code examples that will help you with this part of the assignment:\n",
        "\n",
        "* [Get started for beginners](https://www.tensorflow.org/beta/tutorials/quickstart/beginner)\n",
        "* [Classify images](https://www.tensorflow.org/beta/tutorials/keras/basic_classification)\n",
        "* [Explore overfitting and underfitting](https://www.tensorflow.org/beta/tutorials/keras/overfit_and_underfit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6X5681LTm0jC",
        "colab_type": "text"
      },
      "source": [
        "### Download and prepare a dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHL51OKlmwyw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "34c70220-226f-4d12-a6cd-8e60f5a7a467"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daw-THC6m4u6",
        "colab_type": "text"
      },
      "source": [
        "### Define, train, and evaluate a linear model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHWcSJEQm8jB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(10,activation = \"relu\"),\n",
        "  tf.keras.layers.Dense(20,activation = \"relu\"),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#model.fit(x_train, y_train, epochs=5)\n",
        "#model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94bYNNTAamKO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "0f09791b-03b3-4f0b-c7a8-fe62e6e3db0f"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_6 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                7850      \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 20)                220       \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 10)                210       \n",
            "=================================================================\n",
            "Total params: 8,280\n",
            "Trainable params: 8,280\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyLE9HJbwRDp",
        "colab_type": "text"
      },
      "source": [
        "### 1a: Plot loss and accuracy\n",
        "\n",
        "**TODO**\n",
        "\n",
        "Modify the code below to produce plots showing loss and accuracy as a function of epochs on training and validation data (it's fine to use x_test and y_test as validation data for this assignment). To do so, you will need to add validation data to the call for ```model.fit```, and capture the results in a history object. Code for plotting is provided for you, you can pass your history object to this.\n",
        "\n",
        "![Plot 1](https://storage.googleapis.com/applied-dl/im/a1-1.png)\n",
        "![Plot 2](https://storage.googleapis.com/applied-dl/im/a1-2.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGFjRexynN11",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 951
        },
        "outputId": "49b2849f-f85c-468e-b375-70ec197fd84d"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# 1. create a history object to store the results of model.fit\n",
        "# ```history = model.fit(...)```\n",
        "# 2. add another parameter to model.fit for validation data\n",
        "# https://keras.io/models/sequential/\n",
        "history = model.fit(x_train, y_train, epochs=10, validation_data = (x_test, y_test))\n",
        "\n",
        "# A plotting function you can reuse\n",
        "def plot(history):\n",
        "  \n",
        "  # The history object contains results on the training and test\n",
        "  # sets for each epoch\n",
        "  acc = history.history['accuracy']\n",
        "  val_acc = history.history['val_accuracy']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  # Get the number of epochs\n",
        "  epochs = range(len(acc))\n",
        "\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.plot(epochs, acc, color='blue', label='Train')\n",
        "  plt.plot(epochs, val_acc, color='orange', label='Val')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  _ = plt.figure()\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.plot(epochs, loss, color='blue', label='Train')\n",
        "  plt.plot(epochs, val_loss, color='orange', label='Val')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  \n",
        "plot(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4690 - accuracy: 0.8778 - val_loss: 0.3069 - val_accuracy: 0.9156\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 6s 101us/sample - loss: 0.3035 - accuracy: 0.9151 - val_loss: 0.2811 - val_accuracy: 0.9216\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.2828 - accuracy: 0.9208 - val_loss: 0.2709 - val_accuracy: 0.9235\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 6s 101us/sample - loss: 0.2730 - accuracy: 0.9240 - val_loss: 0.2744 - val_accuracy: 0.9224\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 6s 100us/sample - loss: 0.2667 - accuracy: 0.9256 - val_loss: 0.2657 - val_accuracy: 0.9261\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.2618 - accuracy: 0.9272 - val_loss: 0.2627 - val_accuracy: 0.9260\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 6s 108us/sample - loss: 0.2586 - accuracy: 0.9280 - val_loss: 0.2613 - val_accuracy: 0.9283\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 6s 108us/sample - loss: 0.2553 - accuracy: 0.9288 - val_loss: 0.2703 - val_accuracy: 0.9256\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.2528 - accuracy: 0.9300 - val_loss: 0.2680 - val_accuracy: 0.9258\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.2511 - accuracy: 0.9311 - val_loss: 0.2663 - val_accuracy: 0.9274\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EhrKNT8wfgD",
        "colab_type": "text"
      },
      "source": [
        "### 1b: Implement a neural network \n",
        "\n",
        "**TODO**\n",
        "\n",
        "Modify the code below to create a neural network (with a single hidden layer). Add a Dense layer with 128 units and ReLU activation. Train and evaluate your model. How does the accuracy compare with the linear model above? It is not necessary to produce plots for this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYUgpSwPq-HF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "947072a8-9163-4418-e333-4ebd0bfa4c2b"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128,activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 6s 99us/sample - loss: 0.2579 - accuracy: 0.9264\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 6s 93us/sample - loss: 0.1112 - accuracy: 0.9670\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 6s 93us/sample - loss: 0.0750 - accuracy: 0.9775\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 6s 93us/sample - loss: 0.0574 - accuracy: 0.9825\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 6s 93us/sample - loss: 0.0443 - accuracy: 0.9862\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2837519f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWKtuLSFDjrE",
        "colab_type": "text"
      },
      "source": [
        "**The accuracy increased significantly**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V18pDvPzwh6l",
        "colab_type": "text"
      },
      "source": [
        "### 1c: Implement a deep neural network \n",
        "\n",
        "**TODO**\n",
        "\n",
        "Modify the code below to create and train a deep neural network with at least two hidden layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0odgGhmrNNh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "05e275e6-6110-4ee5-fca2-43ce6fc8848b"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128,activation='relu'),\n",
        "  tf.keras.layers.Dense(128,activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-20d23d8315ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model = tf.keras.models.Sequential([\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkmnVGG-wnyw",
        "colab_type": "text"
      },
      "source": [
        "### 1d: Display predictions and their confidence \n",
        "\n",
        "**TODO**\n",
        "\n",
        "1. Choose one of your models above. Use it to make predictions on the entire test set using ```model.predict```\n",
        "\n",
        "2. Next, identify one image from the testing set the model classifies correctly, and another that it classifies incorrectly. Add code to display these images below, the correct labels, the predicted labels, and the confidence scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPd5NvaV1XEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128,activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "predict = model.predict(x_test)\n",
        "check_T = True\n",
        "check_F = True\n",
        "temp = False\n",
        "fig,ax = plt.subplots(1,2)\n",
        "for i in range(predict.shape[0]):\n",
        "  if np.argmax(predict[i]) == y_test[i] and check_T:\n",
        "    print('Correct Prediction: ','\\nPrediction = ',np.argmax(predict[i]),'\\nData = ', predict[i],' ', '\\nCorrect Answer = ', y_test[i])\n",
        "    ax[0].imshow(x_test[i])\n",
        "    check_T = False\n",
        "    temp = True\n",
        "    continue\n",
        "  if np.argmax(predict[i]) != y_test[i] and check_F and temp:\n",
        "    print('False Prediction: ','\\nPrediction = ',np.argmax(predict[i]),'\\nData = ', predict[i],' ', '\\nCorrect Answer = ', y_test[i])\n",
        "    ax[1].imshow(x_test[i])\n",
        "    check_F = False\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca0nFGJdv3H2",
        "colab_type": "text"
      },
      "source": [
        "## Part 2: Subclassed models\n",
        "\n",
        "In this part of the assignment, you'll work with the Keras Subclassing API. Instead of using a built-in method (```model.fit```) you will train models using a GradientTape.\n",
        "\n",
        "Here are a few code examples that will help you with this part of the assignment:\n",
        "\n",
        "* [Get started for experts](https://www.tensorflow.org/beta/tutorials/quickstart/advanced)\n",
        "* [Tensors and operations](https://www.tensorflow.org/beta/tutorials/eager/basics)\n",
        "* [Keras overview](https://www.tensorflow.org/beta/guide/keras/overview)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB6r75B5teHv",
        "colab_type": "text"
      },
      "source": [
        "### Download and prepare a dataset\n",
        "This is similar to the above, except now we'll use ```tf.data``` to batch and shuffle the data, instead of the utilities baked into ```model.fit```. It's not necessary for this assignment, but if you wish, you can learn how to use tf.data [here](https://www.tensorflow.org/beta/tutorials/load_data/images)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYbkeJJqtm0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download a dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Batch and shuffle the data\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train.astype('float32') / 255, y_train)).shuffle(1024).batch(32)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_test.astype('float32') / 255, y_test)).batch(32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JlSs-3qvCgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A quick example of iterating over a dataset object\n",
        "for image, label in train_ds.take(1):\n",
        "  plt.imshow(image[0], plt.get_cmap('gray'))\n",
        "  print(label[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Nmw1h6stTg5",
        "colab_type": "text"
      },
      "source": [
        "### Define and train a linear model\n",
        "\n",
        "You may see some warnings running the below code (that's okay, just a matter of TF 2.0 being under active development)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWGJoufMtbHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyLinearModel(Model):\n",
        "  def __init__(self):\n",
        "    super(MyLinearModel, self).__init__()\n",
        "    self.flatten = Flatten()\n",
        "    self.d1 = Dense(10, activation='softmax', name=\"dense1\")\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    return self.d1(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35tIz37EttdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = MyLinearModel()\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# For each epoch\n",
        "for epoch in range(5):\n",
        "\n",
        "  # For each batch of images and labels\n",
        "  for images, labels in train_ds:\n",
        "\n",
        "    # Open a GradientTape.\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "      # Forward pass\n",
        "      predictions = model(images)\n",
        "\n",
        "      # Calculate loss\n",
        "      loss = loss_fn(labels, predictions)\n",
        "\n",
        "    # Backprop to calculate gradients\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    # Gradient descent step\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    \n",
        "  print(\"Epoch {}, Loss: {}\".format(epoch, loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEO51Mvi1jOT",
        "colab_type": "text"
      },
      "source": [
        "Note: you may have noticed that the above code runs slowly (it's executing eagerly). Later in this notebook, you will compile your code (to run it in graph mode) using ```@tf.function```. The general workflow is to write your code without using tf.function (as shown above, which makes for easier debugging). Once you've finished debugging your model, you can add ```@tf.function``` for performance if necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj7Y8JWgwv9Z",
        "colab_type": "text"
      },
      "source": [
        "### 2a: Visualize the learned weights\n",
        "\n",
        "We can interpret a linear model by looking at the weights of the fully connected layer. Modify the below code to create a plot similar to the following:\n",
        "\n",
        "![Plot of weights](https://storage.googleapis.com/applied-dl/im/a1-3.png)\n",
        "\n",
        "If you find this section tricky, it is not necessary to complete it to continue with the rest of the assignment.\n",
        "\n",
        "\n",
        "**TODO**\n",
        "\n",
        "Modify the below code to retrieve the learned weights. You can use either the public API of a model ```model.get_layer(name)``` then retrieve the weights from that, or (because our model is defined using the Subclassing API), you can access the dense layer directly ```model.d1```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiQZV03RuWHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# There are two ways to retrieve the weights. You can use the public API\n",
        "# (model.get_layer(name).get_weights()), or, you can access the dense layer \n",
        "# directly (model.dl) then find the accessor method, or again, access the\n",
        "# variable directly.\n",
        "# Python tip: try ```dir(model.d1)```\n",
        "\n",
        "# Modify me\n",
        "weights, bias = model.get_layer('dense1').get_weights()\n",
        "print(weights)\n",
        "print(weights.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz3VV2EiuY7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axs = plt.subplots(1,10, figsize=(20,20))\n",
        "for i in range(10):\n",
        "  subplot = axs[i]\n",
        "  subplot.set_title(i)\n",
        "  subplot.axis('off')\n",
        "  # Modify me\n",
        "  i_weights = weights[:,i] # Select the weights for the i'th output\n",
        "  img = i_weights.reshape(28,28) # Reshape the weights into a 28x28 array\n",
        "  subplot.imshow(img, plt.get_cmap('seismic'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNBCIqzdw2GS",
        "colab_type": "text"
      },
      "source": [
        "### 2b: Implement a deep neural network\n",
        "\n",
        "**TODO**\n",
        "\n",
        "Modify this code to create a deep neural network. Train your model using the code below, and compare the accuracy to the linear model above. \n",
        "\n",
        "Note: you do not need to modify any sections other than the model definition. \n",
        "\n",
        "The code below uses compiled versions of the training and evaluation loops (remove the ```@tf.function``` annotations if you need to debug)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxoIfq91xDRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyDNN(Model):\n",
        "  def __init__(self):\n",
        "    super(MyDNN, self).__init__()\n",
        "    self.flatten = Flatten()\n",
        "    # Modify me\n",
        "    self.d1 = Dense(128,activation='relu')\n",
        "    self.d2 = Dense(10, activation='softmax')\n",
        "    \n",
        "  def call(self, x):\n",
        "\n",
        "    r = self.flatten(x)\n",
        "    r = self.d1(r)\n",
        "    print(r.shape)\n",
        "    return self.d2(r)\n",
        "  \n",
        "model = MyDNN()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVDDq2orxPBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNC0K6JPxy-g",
        "colab_type": "text"
      },
      "source": [
        "These are helper functions we'll use to record loss and accuracy while your model is trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duYKrACkxRHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_oUQki5x0lD",
        "colab_type": "text"
      },
      "source": [
        "This method trains the model on a batch of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTQAnrOAxUAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(images)\n",
        "    loss = loss_object(labels, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lUJFkfux44o",
        "colab_type": "text"
      },
      "source": [
        "This method evaluates the model on a batch of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTMp9PwZxV5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  predictions = model(images)\n",
        "  t_loss = loss_object(labels, predictions)\n",
        "\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR0_bNd-x6OE",
        "colab_type": "text"
      },
      "source": [
        "Training and evaluation loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enHqlaCfxXq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  for images, labels in train_ds:\n",
        "    train_step(images, labels)\n",
        "\n",
        "  for test_images, test_labels in test_ds:\n",
        "    test_step(test_images, test_labels)\n",
        "\n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "  print(template.format(epoch+1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result()*100,\n",
        "                        test_loss.result(),\n",
        "                        test_accuracy.result()*100))\n",
        "\n",
        "  # Reset the metrics for the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxjIFiyBPorB",
        "colab_type": "text"
      },
      "source": [
        "### 2c: Provide your own implementation of softmax and use it to train a model\n",
        "\n",
        "In your linear model above, the starter code looked similar to: \n",
        "\n",
        "\n",
        "```\n",
        "class LinearModel(Model):\n",
        "  def __init__(self):\n",
        "    super(LinearModel, self).__init__()\n",
        "    self.flatten = Flatten()\n",
        "    self.d1 = Dense(10, activation='softmax')\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    return self.d1(x)\n",
        "``` \n",
        "\n",
        "Now, create a function:\n",
        "\n",
        "\n",
        "```\n",
        "def my_softmax(logits):\n",
        "  # ...\n",
        "```\n",
        "\n",
        "and use it in your model as follows:\n",
        "\n",
        "\n",
        "```\n",
        "class LinearModel(Model):\n",
        "  def __init__(self):\n",
        "    super(LinearModel, self).__init__()\n",
        "    self.flatten = Flatten()\n",
        "    self.d1 = Dense(10)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    return my_softmax(x)\n",
        "``` \n",
        "\n",
        "Notice, we've removed the built-in activation method on the Dense layer, and added our own to the call method.\n",
        "\n",
        "Tip: You can implement softmax first using NumPy, if you like, the gradually convert your code to use TensorFlow ops (which begin with tf.\\* instead of np.\\*).\n",
        "\n",
        "Notes: \n",
        "\n",
        "- Your softmax implementation should be numerically stable. \n",
        "- You will need to use tf.* ops in order to use your code to train a model (TF cannot backprop through NumPy operations)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiLj-uFrRAn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: your code here\n",
        "\n",
        "'''\n",
        "class MyDNN(Model):\n",
        "  def __init__(self):\n",
        "    super(MyDNN, self).__init__()\n",
        "    self.flatten = Flatten()\n",
        "    # Modify me\n",
        "    self.d1 = Dense(10, activation='softmax')\n",
        "    self.d2 = Dense(10, activation='softmax')\n",
        "\n",
        "  def call(self, x):\n",
        "    r = self.flatten(x)\n",
        "    r = self.d1(r)\n",
        "    print(r.shape)\n",
        "    return self.d2(r)\n",
        "\n",
        "model = MyDNN()\n",
        "'''\n",
        "'''\n",
        "class LinearModel(Model):\n",
        "  def __init__(self):\n",
        "    super(LinearModel, self).__init__()\n",
        "    self.flatten = Flatten()\n",
        "    self.d1 = Dense(10)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    return my_softmax(x)\n",
        "'''\n",
        "def my_softmax(x): \n",
        "  result = tf.exp(x) / tf.reduce_sum(tf.exp(x), 0)\n",
        "  print(result.shape,'134')\n",
        "  '''\n",
        "  height,width = x.shape\n",
        "  result = np.zeros(x.shape)\n",
        "  print(result.shape)\n",
        "  '''\n",
        "  '''\n",
        "  for i in range(height):\n",
        "    sum_0 = np.sum(tf.exp(x_0[i],0))\n",
        "    for j in range(width):\n",
        "      result[i,j] = tf.exp((x_0[i,j]))/sum_0\n",
        "      print(result[i,j])\n",
        "  '''\n",
        "  return result\n",
        "\n",
        "class LinearModel(Model):\n",
        "  def __init__(self):\n",
        "    super(LinearModel, self).__init__()\n",
        "    self.flatten = Flatten()\n",
        "    self.d1 = Dense(10)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    return my_softmax(x)\n",
        "\n",
        "model = LinearModel()\n",
        "\n",
        "\n",
        "#############################\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "#############################\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "############################\n",
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(images)\n",
        "    loss = loss_object(labels, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels, predictions)\n",
        "############################\n",
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  predictions = model(images)\n",
        "  t_loss = loss_object(labels, predictions)\n",
        "\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, predictions)\n",
        "############################\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  for images, labels in train_ds:\n",
        "    train_step(images, labels)\n",
        "\n",
        "  for test_images, test_labels in test_ds:\n",
        "    test_step(test_images, test_labels)\n",
        "\n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "  print(template.format(epoch+1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result()*100,\n",
        "                        test_loss.result(),\n",
        "                        test_accuracy.result()*100))\n",
        "\n",
        "  # Reset the metrics for the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states() \n",
        "\n",
        "# TODO\n",
        "# Add code to train you model, your accuracy should be similar \n",
        "# to the linear model.\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}